{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: Low-Rank Adaptation (LoRA).\n",
    "* Model: DistilBERT\n",
    "* Evaluation approach: evaluate method with a Hugging Face Trainer\n",
    "* Fine-tuning dataset: https://huggingface.co/datasets/stanfordnlp/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e812837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is based on the solution to the exercise Create a BERT Sentiment Classifier.\n",
    "#Adaptation of the DistilBERT model by probing it with a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Collecting huggingface-hub>=0.23.0\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tqdm, requests, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.21.4\n",
      "    Uninstalling huggingface-hub-0.21.4:\n",
      "      Successfully uninstalled huggingface-hub-0.21.4\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed datasets-3.2.0 huggingface-hub-0.27.0 requests-2.32.3 tqdm-4.67.1\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Install datasets. You may need to restart the kernel after installation.\n",
    "!pip install -U datasets\n",
    "!pip install -q \"datasets==2.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f12c5224164c4a8674accb1367d2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c83ce2d81044a0db210b2cb033abbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4494efc8b50b4bf2a4530a69a664d1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99db908ae9d3468ab5a40ce052dfc8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d708395764d74b988c9a5de57ae1ca43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd051e38cbe4eb38ae1bfe4b55d96af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b483735d82247d5887a9e6a6a40ae78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe68d3c57d8746c5a96c4f4da5cc651b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b88f69b03644cf195d53bd4daccd1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import the datasets.\n",
    "from datasets import load_dataset\n",
    "\n",
    "#Load the train and test splits of the imdb dataset and store them in a dictionary.\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"imdb\", split=splits))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3313c99d6e40228fdadeafbee1c4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb32180efd94d6999b7fe7f6bccccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e475158ff6c432693bc4411ef72e8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bdd67850d94142ba386072a3e2cde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf91111c5794114a47aff294af60781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f20f4394f91455d8b4a60c66fde0a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n",
       "  \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\"],\n",
       " 'label': [0, 0, 0],\n",
       " 'input_ids': [[101,\n",
       "   1045,\n",
       "   12524,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2013,\n",
       "   2026,\n",
       "   2678,\n",
       "   3573,\n",
       "   2138,\n",
       "   1997,\n",
       "   2035,\n",
       "   1996,\n",
       "   6704,\n",
       "   2008,\n",
       "   5129,\n",
       "   2009,\n",
       "   2043,\n",
       "   2009,\n",
       "   2001,\n",
       "   2034,\n",
       "   2207,\n",
       "   1999,\n",
       "   3476,\n",
       "   1012,\n",
       "   1045,\n",
       "   2036,\n",
       "   2657,\n",
       "   2008,\n",
       "   2012,\n",
       "   2034,\n",
       "   2009,\n",
       "   2001,\n",
       "   8243,\n",
       "   2011,\n",
       "   1057,\n",
       "   1012,\n",
       "   1055,\n",
       "   1012,\n",
       "   8205,\n",
       "   2065,\n",
       "   2009,\n",
       "   2412,\n",
       "   2699,\n",
       "   2000,\n",
       "   4607,\n",
       "   2023,\n",
       "   2406,\n",
       "   1010,\n",
       "   3568,\n",
       "   2108,\n",
       "   1037,\n",
       "   5470,\n",
       "   1997,\n",
       "   3152,\n",
       "   2641,\n",
       "   1000,\n",
       "   6801,\n",
       "   1000,\n",
       "   1045,\n",
       "   2428,\n",
       "   2018,\n",
       "   2000,\n",
       "   2156,\n",
       "   2023,\n",
       "   2005,\n",
       "   2870,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1996,\n",
       "   5436,\n",
       "   2003,\n",
       "   8857,\n",
       "   2105,\n",
       "   1037,\n",
       "   2402,\n",
       "   4467,\n",
       "   3689,\n",
       "   3076,\n",
       "   2315,\n",
       "   14229,\n",
       "   2040,\n",
       "   4122,\n",
       "   2000,\n",
       "   4553,\n",
       "   2673,\n",
       "   2016,\n",
       "   2064,\n",
       "   2055,\n",
       "   2166,\n",
       "   1012,\n",
       "   1999,\n",
       "   3327,\n",
       "   2016,\n",
       "   4122,\n",
       "   2000,\n",
       "   3579,\n",
       "   2014,\n",
       "   3086,\n",
       "   2015,\n",
       "   2000,\n",
       "   2437,\n",
       "   2070,\n",
       "   4066,\n",
       "   1997,\n",
       "   4516,\n",
       "   2006,\n",
       "   2054,\n",
       "   1996,\n",
       "   2779,\n",
       "   25430,\n",
       "   14728,\n",
       "   2245,\n",
       "   2055,\n",
       "   3056,\n",
       "   2576,\n",
       "   3314,\n",
       "   2107,\n",
       "   2004,\n",
       "   1996,\n",
       "   5148,\n",
       "   2162,\n",
       "   1998,\n",
       "   2679,\n",
       "   3314,\n",
       "   1999,\n",
       "   1996,\n",
       "   2142,\n",
       "   2163,\n",
       "   1012,\n",
       "   1999,\n",
       "   2090,\n",
       "   4851,\n",
       "   8801,\n",
       "   1998,\n",
       "   6623,\n",
       "   7939,\n",
       "   4697,\n",
       "   3619,\n",
       "   1997,\n",
       "   8947,\n",
       "   2055,\n",
       "   2037,\n",
       "   10740,\n",
       "   2006,\n",
       "   4331,\n",
       "   1010,\n",
       "   2016,\n",
       "   2038,\n",
       "   3348,\n",
       "   2007,\n",
       "   2014,\n",
       "   3689,\n",
       "   3836,\n",
       "   1010,\n",
       "   19846,\n",
       "   1010,\n",
       "   1998,\n",
       "   2496,\n",
       "   2273,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2054,\n",
       "   8563,\n",
       "   2033,\n",
       "   2055,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2003,\n",
       "   2008,\n",
       "   2871,\n",
       "   2086,\n",
       "   3283,\n",
       "   1010,\n",
       "   2023,\n",
       "   2001,\n",
       "   2641,\n",
       "   26932,\n",
       "   1012,\n",
       "   2428,\n",
       "   1010,\n",
       "   1996,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   5019,\n",
       "   2024,\n",
       "   2261,\n",
       "   1998,\n",
       "   2521,\n",
       "   2090,\n",
       "   1010,\n",
       "   2130,\n",
       "   2059,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   2025,\n",
       "   2915,\n",
       "   2066,\n",
       "   2070,\n",
       "   10036,\n",
       "   2135,\n",
       "   2081,\n",
       "   22555,\n",
       "   2080,\n",
       "   1012,\n",
       "   2096,\n",
       "   2026,\n",
       "   2406,\n",
       "   3549,\n",
       "   2568,\n",
       "   2424,\n",
       "   2009,\n",
       "   16880,\n",
       "   1010,\n",
       "   1999,\n",
       "   4507,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   2024,\n",
       "   1037,\n",
       "   2350,\n",
       "   18785,\n",
       "   1999,\n",
       "   4467,\n",
       "   5988,\n",
       "   1012,\n",
       "   2130,\n",
       "   13749,\n",
       "   7849,\n",
       "   24544,\n",
       "   1010,\n",
       "   15835,\n",
       "   2037,\n",
       "   3437,\n",
       "   2000,\n",
       "   2204,\n",
       "   2214,\n",
       "   2879,\n",
       "   2198,\n",
       "   4811,\n",
       "   1010,\n",
       "   2018,\n",
       "   3348,\n",
       "   5019,\n",
       "   1999,\n",
       "   2010,\n",
       "   3152,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1045,\n",
       "   2079,\n",
       "   4012,\n",
       "   3549,\n",
       "   2094,\n",
       "   1996,\n",
       "   16587,\n",
       "   2005,\n",
       "   1996,\n",
       "   2755,\n",
       "   2008,\n",
       "   2151,\n",
       "   3348,\n",
       "   3491,\n",
       "   1999,\n",
       "   1996,\n",
       "   2143,\n",
       "   2003,\n",
       "   3491,\n",
       "   2005,\n",
       "   6018,\n",
       "   5682,\n",
       "   2738,\n",
       "   2084,\n",
       "   2074,\n",
       "   2000,\n",
       "   5213,\n",
       "   2111,\n",
       "   1998,\n",
       "   2191,\n",
       "   2769,\n",
       "   2000,\n",
       "   2022,\n",
       "   3491,\n",
       "   1999,\n",
       "   26932,\n",
       "   12370,\n",
       "   1999,\n",
       "   2637,\n",
       "   1012,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2003,\n",
       "   1037,\n",
       "   2204,\n",
       "   2143,\n",
       "   2005,\n",
       "   3087,\n",
       "   5782,\n",
       "   2000,\n",
       "   2817,\n",
       "   1996,\n",
       "   6240,\n",
       "   1998,\n",
       "   14629,\n",
       "   1006,\n",
       "   2053,\n",
       "   26136,\n",
       "   3832,\n",
       "   1007,\n",
       "   1997,\n",
       "   4467,\n",
       "   5988,\n",
       "   1012,\n",
       "   2021,\n",
       "   2428,\n",
       "   1010,\n",
       "   2023,\n",
       "   2143,\n",
       "   2987,\n",
       "   1005,\n",
       "   1056,\n",
       "   2031,\n",
       "   2172,\n",
       "   1997,\n",
       "   1037,\n",
       "   5436,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   1000,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1024,\n",
       "   3756,\n",
       "   1000,\n",
       "   2003,\n",
       "   1037,\n",
       "   15544,\n",
       "   19307,\n",
       "   1998,\n",
       "   3653,\n",
       "   6528,\n",
       "   20771,\n",
       "   19986,\n",
       "   8632,\n",
       "   1012,\n",
       "   2009,\n",
       "   2987,\n",
       "   1005,\n",
       "   1056,\n",
       "   3043,\n",
       "   2054,\n",
       "   2028,\n",
       "   1005,\n",
       "   1055,\n",
       "   2576,\n",
       "   5328,\n",
       "   2024,\n",
       "   2138,\n",
       "   2023,\n",
       "   2143,\n",
       "   2064,\n",
       "   6684,\n",
       "   2022,\n",
       "   2579,\n",
       "   5667,\n",
       "   2006,\n",
       "   2151,\n",
       "   2504,\n",
       "   1012,\n",
       "   2004,\n",
       "   2005,\n",
       "   1996,\n",
       "   4366,\n",
       "   2008,\n",
       "   19124,\n",
       "   3287,\n",
       "   16371,\n",
       "   25469,\n",
       "   2003,\n",
       "   2019,\n",
       "   6882,\n",
       "   13316,\n",
       "   1011,\n",
       "   2459,\n",
       "   1010,\n",
       "   2008,\n",
       "   3475,\n",
       "   1005,\n",
       "   1056,\n",
       "   2995,\n",
       "   1012,\n",
       "   1045,\n",
       "   1005,\n",
       "   2310,\n",
       "   2464,\n",
       "   1054,\n",
       "   1011,\n",
       "   6758,\n",
       "   3152,\n",
       "   2007,\n",
       "   3287,\n",
       "   16371,\n",
       "   25469,\n",
       "   1012,\n",
       "   4379,\n",
       "   1010,\n",
       "   2027,\n",
       "   2069,\n",
       "   3749,\n",
       "   2070,\n",
       "   25085,\n",
       "   5328,\n",
       "   1010,\n",
       "   2021,\n",
       "   2073,\n",
       "   2024,\n",
       "   1996,\n",
       "   1054,\n",
       "   1011,\n",
       "   6758,\n",
       "   3152,\n",
       "   2007,\n",
       "   21226,\n",
       "   24728,\n",
       "   22144,\n",
       "   2015,\n",
       "   1998,\n",
       "   20916,\n",
       "   4691,\n",
       "   6845,\n",
       "   2401,\n",
       "   1029,\n",
       "   7880,\n",
       "   1010,\n",
       "   2138,\n",
       "   2027,\n",
       "   2123,\n",
       "   1005,\n",
       "   1056,\n",
       "   4839,\n",
       "   1012,\n",
       "   1996,\n",
       "   2168,\n",
       "   3632,\n",
       "   2005,\n",
       "   2216,\n",
       "   10231,\n",
       "   7685,\n",
       "   5830,\n",
       "   3065,\n",
       "   1024,\n",
       "   8040,\n",
       "   7317,\n",
       "   5063,\n",
       "   2015,\n",
       "   11820,\n",
       "   1999,\n",
       "   1996,\n",
       "   9478,\n",
       "   2021,\n",
       "   2025,\n",
       "   1037,\n",
       "   17962,\n",
       "   21239,\n",
       "   1999,\n",
       "   4356,\n",
       "   1012,\n",
       "   1998,\n",
       "   2216,\n",
       "   3653,\n",
       "   6528,\n",
       "   20771,\n",
       "   10271,\n",
       "   5691,\n",
       "   2066,\n",
       "   1996,\n",
       "   2829,\n",
       "   16291,\n",
       "   1010,\n",
       "   1999,\n",
       "   2029,\n",
       "   2057,\n",
       "   1005,\n",
       "   2128,\n",
       "   5845,\n",
       "   2000,\n",
       "   1996,\n",
       "   2609,\n",
       "   1997,\n",
       "   6320,\n",
       "   25624,\n",
       "   1005,\n",
       "   1055,\n",
       "   17061,\n",
       "   3779,\n",
       "   1010,\n",
       "   2021,\n",
       "   2025,\n",
       "   1037,\n",
       "   7637,\n",
       "   1997,\n",
       "   5061,\n",
       "   5710,\n",
       "   2006,\n",
       "   9318,\n",
       "   7367,\n",
       "   5737,\n",
       "   19393,\n",
       "   1012,\n",
       "   2077,\n",
       "   6933,\n",
       "   1006,\n",
       "   2030,\n",
       "   20242,\n",
       "   1007,\n",
       "   1000,\n",
       "   3313,\n",
       "   1011,\n",
       "   3115,\n",
       "   1000,\n",
       "   1999,\n",
       "   5609,\n",
       "   1997,\n",
       "   16371,\n",
       "   25469,\n",
       "   1010,\n",
       "   1996,\n",
       "   10597,\n",
       "   27885,\n",
       "   5809,\n",
       "   2063,\n",
       "   2323,\n",
       "   2202,\n",
       "   2046,\n",
       "   4070,\n",
       "   2028,\n",
       "   14477,\n",
       "   6767,\n",
       "   8524,\n",
       "   6321,\n",
       "   5793,\n",
       "   28141,\n",
       "   4489,\n",
       "   2090,\n",
       "   2273,\n",
       "   1998,\n",
       "   2308,\n",
       "   1024,\n",
       "   2045,\n",
       "   2024,\n",
       "   2053,\n",
       "   8991,\n",
       "   18400,\n",
       "   2015,\n",
       "   2006,\n",
       "   4653,\n",
       "   2043,\n",
       "   19910,\n",
       "   3544,\n",
       "   15287,\n",
       "   1010,\n",
       "   1998,\n",
       "   1996,\n",
       "   2168,\n",
       "   3685,\n",
       "   2022,\n",
       "   2056,\n",
       "   2005,\n",
       "   1037,\n",
       "   2158,\n",
       "   1012,\n",
       "   1999,\n",
       "   2755,\n",
       "   1010,\n",
       "   2017,\n",
       "   3227,\n",
       "   2180,\n",
       "   1005,\n",
       "   1056,\n",
       "   2156,\n",
       "   2931,\n",
       "   8991,\n",
       "   18400,\n",
       "   2015,\n",
       "   1999,\n",
       "   2019,\n",
       "   2137,\n",
       "   2143,\n",
       "   1999,\n",
       "   2505,\n",
       "   2460,\n",
       "   1997,\n",
       "   22555,\n",
       "   2030,\n",
       "   13216,\n",
       "   14253,\n",
       "   2050,\n",
       "   1012,\n",
       "   2023,\n",
       "   6884,\n",
       "   3313,\n",
       "   1011,\n",
       "   3115,\n",
       "   2003,\n",
       "   2625,\n",
       "   1037,\n",
       "   3313,\n",
       "   3115,\n",
       "   2084,\n",
       "   2019,\n",
       "   4914,\n",
       "   2135,\n",
       "   2139,\n",
       "   24128,\n",
       "   3754,\n",
       "   2000,\n",
       "   2272,\n",
       "   2000,\n",
       "   3408,\n",
       "   20547,\n",
       "   2007,\n",
       "   1996,\n",
       "   19008,\n",
       "   1997,\n",
       "   2308,\n",
       "   1005,\n",
       "   1055,\n",
       "   4230,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2065,\n",
       "   2069,\n",
       "   2000,\n",
       "   4468,\n",
       "   2437,\n",
       "   2023,\n",
       "   2828,\n",
       "   1997,\n",
       "   2143,\n",
       "   1999,\n",
       "   1996,\n",
       "   2925,\n",
       "   1012,\n",
       "   2023,\n",
       "   2143,\n",
       "   2003,\n",
       "   5875,\n",
       "   2004,\n",
       "   2019,\n",
       "   7551,\n",
       "   2021,\n",
       "   4136,\n",
       "   2053,\n",
       "   2522,\n",
       "   11461,\n",
       "   2466,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2028,\n",
       "   2453,\n",
       "   2514,\n",
       "   6819,\n",
       "   5339,\n",
       "   8918,\n",
       "   2005,\n",
       "   3564,\n",
       "   27046,\n",
       "   2009,\n",
       "   2138,\n",
       "   2009,\n",
       "   12817,\n",
       "   2006,\n",
       "   2061,\n",
       "   2116,\n",
       "   2590,\n",
       "   3314,\n",
       "   2021,\n",
       "   2009,\n",
       "   2515,\n",
       "   2061,\n",
       "   2302,\n",
       "   2151,\n",
       "   5860,\n",
       "   11795,\n",
       "   3085,\n",
       "   15793,\n",
       "   1012,\n",
       "   1996,\n",
       "   13972,\n",
       "   3310,\n",
       "   2185,\n",
       "   2007,\n",
       "   2053,\n",
       "   2047,\n",
       "   15251,\n",
       "   1006,\n",
       "   4983,\n",
       "   2028,\n",
       "   3310,\n",
       "   2039,\n",
       "   2007,\n",
       "   2028,\n",
       "   2096,\n",
       "   2028,\n",
       "   1005,\n",
       "   1055,\n",
       "   2568,\n",
       "   17677,\n",
       "   2015,\n",
       "   1010,\n",
       "   2004,\n",
       "   2009,\n",
       "   2097,\n",
       "   26597,\n",
       "   2079,\n",
       "   2076,\n",
       "   2023,\n",
       "   23100,\n",
       "   2143,\n",
       "   1007,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2028,\n",
       "   2453,\n",
       "   2488,\n",
       "   5247,\n",
       "   2028,\n",
       "   1005,\n",
       "   1055,\n",
       "   2051,\n",
       "   4582,\n",
       "   2041,\n",
       "   1037,\n",
       "   3332,\n",
       "   2012,\n",
       "   1037,\n",
       "   3392,\n",
       "   3652,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   102]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import and set up a tokeniser.\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "#Let's use a lambda function to tokenize both datasets (train and test).\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = ds[split].map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "#Inspect the first three examples in the train dataset after tokenisation.\n",
    "tokenized_dataset[\"train\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d765dc4372463fb35c489b7336c05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Import the DistilBERT pretrained model from HF and define it as our base model.\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Freeze DistilBERT's parameters. Note that the classification head's weights are still trainable.\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Inspect the model.\n",
    "model.classifier\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5273594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 19:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6918345093727112,\n",
       " 'eval_accuracy': 0.52436,\n",
       " 'eval_runtime': 350.0246,\n",
       " 'eval_samples_per_second': 71.424,\n",
       " 'eval_steps_per_second': 17.856}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the libraries necessary for training the classification head.\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "#This function calculates the classifier's performance with respect to a dataset in terms of the accuracy metric.\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "#The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "#Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory.\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#Evaluate the pretrained classifier. The trainer loop above is also an evaluation loop.\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049b39f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 12:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.351600</td>\n",
       "      <td>0.352573</td>\n",
       "      <td>0.860720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-6250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35257336497306824,\n",
       " 'eval_accuracy': 0.86072,\n",
       " 'eval_runtime': 356.2403,\n",
       " 'eval_samples_per_second': 70.177,\n",
       " 'eval_steps_per_second': 17.544,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiate the training loop defined above to train the classification head.\n",
    "trainer.train()\n",
    "\n",
    "#Save the trained classifier.\n",
    "trainer.save_model(\"model_probing\")\n",
    "\n",
    "#Evaluate the trained classifier. \n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distilbert\n",
      "distilbert.embeddings\n",
      "distilbert.embeddings.word_embeddings\n",
      "distilbert.embeddings.position_embeddings\n",
      "distilbert.embeddings.LayerNorm\n",
      "distilbert.embeddings.dropout\n",
      "distilbert.transformer\n",
      "distilbert.transformer.layer\n",
      "distilbert.transformer.layer.0\n",
      "distilbert.transformer.layer.0.attention\n",
      "distilbert.transformer.layer.0.attention.dropout\n",
      "distilbert.transformer.layer.0.attention.q_lin\n",
      "distilbert.transformer.layer.0.attention.k_lin\n",
      "distilbert.transformer.layer.0.attention.v_lin\n",
      "distilbert.transformer.layer.0.attention.out_lin\n",
      "distilbert.transformer.layer.0.sa_layer_norm\n",
      "distilbert.transformer.layer.0.ffn\n",
      "distilbert.transformer.layer.0.ffn.dropout\n",
      "distilbert.transformer.layer.0.ffn.lin1\n",
      "distilbert.transformer.layer.0.ffn.lin2\n",
      "distilbert.transformer.layer.0.ffn.activation\n",
      "distilbert.transformer.layer.0.output_layer_norm\n",
      "distilbert.transformer.layer.1\n",
      "distilbert.transformer.layer.1.attention\n",
      "distilbert.transformer.layer.1.attention.dropout\n",
      "distilbert.transformer.layer.1.attention.q_lin\n",
      "distilbert.transformer.layer.1.attention.k_lin\n",
      "distilbert.transformer.layer.1.attention.v_lin\n",
      "distilbert.transformer.layer.1.attention.out_lin\n",
      "distilbert.transformer.layer.1.sa_layer_norm\n",
      "distilbert.transformer.layer.1.ffn\n",
      "distilbert.transformer.layer.1.ffn.dropout\n",
      "distilbert.transformer.layer.1.ffn.lin1\n",
      "distilbert.transformer.layer.1.ffn.lin2\n",
      "distilbert.transformer.layer.1.ffn.activation\n",
      "distilbert.transformer.layer.1.output_layer_norm\n",
      "distilbert.transformer.layer.2\n",
      "distilbert.transformer.layer.2.attention\n",
      "distilbert.transformer.layer.2.attention.dropout\n",
      "distilbert.transformer.layer.2.attention.q_lin\n",
      "distilbert.transformer.layer.2.attention.k_lin\n",
      "distilbert.transformer.layer.2.attention.v_lin\n",
      "distilbert.transformer.layer.2.attention.out_lin\n",
      "distilbert.transformer.layer.2.sa_layer_norm\n",
      "distilbert.transformer.layer.2.ffn\n",
      "distilbert.transformer.layer.2.ffn.dropout\n",
      "distilbert.transformer.layer.2.ffn.lin1\n",
      "distilbert.transformer.layer.2.ffn.lin2\n",
      "distilbert.transformer.layer.2.ffn.activation\n",
      "distilbert.transformer.layer.2.output_layer_norm\n",
      "distilbert.transformer.layer.3\n",
      "distilbert.transformer.layer.3.attention\n",
      "distilbert.transformer.layer.3.attention.dropout\n",
      "distilbert.transformer.layer.3.attention.q_lin\n",
      "distilbert.transformer.layer.3.attention.k_lin\n",
      "distilbert.transformer.layer.3.attention.v_lin\n",
      "distilbert.transformer.layer.3.attention.out_lin\n",
      "distilbert.transformer.layer.3.sa_layer_norm\n",
      "distilbert.transformer.layer.3.ffn\n",
      "distilbert.transformer.layer.3.ffn.dropout\n",
      "distilbert.transformer.layer.3.ffn.lin1\n",
      "distilbert.transformer.layer.3.ffn.lin2\n",
      "distilbert.transformer.layer.3.ffn.activation\n",
      "distilbert.transformer.layer.3.output_layer_norm\n",
      "distilbert.transformer.layer.4\n",
      "distilbert.transformer.layer.4.attention\n",
      "distilbert.transformer.layer.4.attention.dropout\n",
      "distilbert.transformer.layer.4.attention.q_lin\n",
      "distilbert.transformer.layer.4.attention.k_lin\n",
      "distilbert.transformer.layer.4.attention.v_lin\n",
      "distilbert.transformer.layer.4.attention.out_lin\n",
      "distilbert.transformer.layer.4.sa_layer_norm\n",
      "distilbert.transformer.layer.4.ffn\n",
      "distilbert.transformer.layer.4.ffn.dropout\n",
      "distilbert.transformer.layer.4.ffn.lin1\n",
      "distilbert.transformer.layer.4.ffn.lin2\n",
      "distilbert.transformer.layer.4.ffn.activation\n",
      "distilbert.transformer.layer.4.output_layer_norm\n",
      "distilbert.transformer.layer.5\n",
      "distilbert.transformer.layer.5.attention\n",
      "distilbert.transformer.layer.5.attention.dropout\n",
      "distilbert.transformer.layer.5.attention.q_lin\n",
      "distilbert.transformer.layer.5.attention.k_lin\n",
      "distilbert.transformer.layer.5.attention.v_lin\n",
      "distilbert.transformer.layer.5.attention.out_lin\n",
      "distilbert.transformer.layer.5.sa_layer_norm\n",
      "distilbert.transformer.layer.5.ffn\n",
      "distilbert.transformer.layer.5.ffn.dropout\n",
      "distilbert.transformer.layer.5.ffn.lin1\n",
      "distilbert.transformer.layer.5.ffn.lin2\n",
      "distilbert.transformer.layer.5.ffn.activation\n",
      "distilbert.transformer.layer.5.output_layer_norm\n",
      "pre_classifier\n",
      "classifier\n",
      "dropout\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Embeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Embedding(30522, 768, padding_idx=0)\n",
      "Embedding(512, 768)\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Transformer(\n",
      "  (layer): ModuleList(\n",
      "    (0-5): 6 x TransformerBlock(\n",
      "      (attention): MultiHeadSelfAttention(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (ffn): FFN(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0-5): 6 x TransformerBlock(\n",
      "    (attention): MultiHeadSelfAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (ffn): FFN(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "TransformerBlock(\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (ffn): FFN(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      "  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "MultiHeadSelfAttention(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "FFN(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "GELUActivation()\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "TransformerBlock(\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (ffn): FFN(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      "  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "MultiHeadSelfAttention(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "FFN(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "GELUActivation()\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "TransformerBlock(\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (ffn): FFN(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      "  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "MultiHeadSelfAttention(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "FFN(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "GELUActivation()\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "TransformerBlock(\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (ffn): FFN(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      "  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "MultiHeadSelfAttention(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "FFN(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "GELUActivation()\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "TransformerBlock(\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (ffn): FFN(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      "  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "MultiHeadSelfAttention(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "FFN(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "GELUActivation()\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "TransformerBlock(\n",
      "  (attention): MultiHeadSelfAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (ffn): FFN(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      "  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "MultiHeadSelfAttention(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "FFN(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "Dropout(p=0.1, inplace=False)\n",
      "Linear(in_features=768, out_features=3072, bias=True)\n",
      "Linear(in_features=3072, out_features=768, bias=True)\n",
      "GELUActivation()\n",
      "LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "Dropout(p=0.2, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "#Import the DistilBERT pretrained model from HF and define it as our base model.\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ")\n",
    "\n",
    "for name, module in model_base.named_modules():\n",
    "    print(name)\n",
    "    \n",
    "for name, module in model_base.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 36,864 || all params: 67,584,004 || trainable%: 0.054545451317148955\n"
     ]
    }
   ],
   "source": [
    "#Import and create a PEFT adapter configuration for low rank adaptation (LoRA).\n",
    "from peft import LoraConfig, TaskType\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, #Sequence classification task.\n",
    "    inference_mode=False,\n",
    "    r=8, #Rank of the adaptation matrices.\n",
    "    lora_alpha=32, #Weight assigned adaptation.\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"distilbert.transformer.layer.5.attention.q_lin\", \"distilbert.transformer.layer.5.attention.k_lin\", \"distilbert.transformer.layer.5.attention.v_lin\"]\n",
    ")    \n",
    "\n",
    "#Import and create a PEFT adapter configuration for low rank adaptation (LoRA).\n",
    "from peft import get_peft_model\n",
    "model_lora = get_peft_model(model_base, config)\n",
    "\n",
    "#Print the number of trainable parameters.\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6239fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer base_model.model.distilbert.embeddings.word_embeddings.weight is frozen.\n",
      "Layer base_model.model.distilbert.embeddings.position_embeddings.weight is frozen.\n",
      "Layer base_model.model.distilbert.embeddings.LayerNorm.weight is frozen.\n",
      "Layer base_model.model.distilbert.embeddings.LayerNorm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.q_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.q_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.v_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.v_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.q_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.q_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.v_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.v_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.q_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.q_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.v_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.v_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.q_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.q_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.v_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.v_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.q_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.q_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.v_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.v_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.q_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.q_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight is trainable.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight is trainable.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_A.default.weight is trainable.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.k_lin.lora_B.default.weight is trainable.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.v_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.v_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight is trainable.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight is trainable.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight is frozen.\n",
      "Layer base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias is frozen.\n",
      "Layer base_model.model.pre_classifier.original_module.weight is frozen.\n",
      "Layer base_model.model.pre_classifier.original_module.bias is frozen.\n",
      "Layer base_model.model.pre_classifier.modules_to_save.default.weight is frozen.\n",
      "Layer base_model.model.pre_classifier.modules_to_save.default.bias is frozen.\n",
      "Layer base_model.model.classifier.original_module.weight is frozen.\n",
      "Layer base_model.model.classifier.original_module.bias is frozen.\n",
      "Layer base_model.model.classifier.modules_to_save.default.weight is frozen.\n",
      "Layer base_model.model.classifier.modules_to_save.default.bias is frozen.\n"
     ]
    }
   ],
   "source": [
    "#Check that the trainable layers are correct.\n",
    "for name, param in model_lora.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"Layer {name} is frozen.\")\n",
    "    else:\n",
    "        print(f\"Layer {name} is trainable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 13:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.329078</td>\n",
       "      <td>0.873720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-6250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6250, training_loss=0.3763320703125, metrics={'train_runtime': 812.7244, 'train_samples_per_second': 30.761, 'train_steps_per_second': 7.69, 'total_flos': 2812718356586688.0, 'train_loss': 0.3763320703125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the libraries necessary for training the classification head and the LoRA adapter.\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "#This function calculates the classifier's performance with respect to a dataset in terms of the accuracy metric.\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "#The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "#Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer.\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory.\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#Initiate the training loop defined above.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3290780782699585,\n",
       " 'eval_accuracy': 0.87372,\n",
       " 'eval_runtime': 358.5293,\n",
       " 'eval_samples_per_second': 69.729,\n",
       " 'eval_steps_per_second': 17.432,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the trained classifier.\n",
    "model_lora.save_pretrained(\"model_lora\")\n",
    "trainer.save_model(\"model_lora_LW\")\n",
    "\n",
    "#Evaluate the trained classifier. \n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load the PEFT (LoRA) model.\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "model_lora_infer = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"model_lora_LW\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the PEFT model's pad ID to that used by the tokeniser.\n",
    "model_lora_infer.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the evaluation loop.\n",
    "inference = Trainer(\n",
    "    model=model_lora_infer, #Remember to use the PEFT model.\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory.\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation = inference.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6d5e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the finetuned model: {'eval_loss': 0.3290780782699585, 'eval_accuracy': 0.87372, 'eval_runtime': 358.4737, 'eval_samples_per_second': 69.74, 'eval_steps_per_second': 17.435}\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of the finetuned model:\", evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b01cd",
   "metadata": {},
   "source": [
    "The evaluation results of the pretrained model adapted with linear probing only are shown in Out[7]. They are reproduced here.\n",
    "\n",
    "{'eval_loss': 0.6918345093727112,\n",
    " 'eval_accuracy': 0.52436,\n",
    " 'eval_runtime': 350.0246,\n",
    " 'eval_samples_per_second': 71.424,\n",
    " 'eval_steps_per_second': 17.856}\n",
    " \n",
    "The evaluation results of the pretrained model adapted with linear probing, which was finetuned with the base model's pretrained weights frozen, are shown in Out[8]. They are reproduced here.\n",
    "\n",
    "{'eval_loss': 0.35257336497306824,\n",
    " 'eval_accuracy': 0.86072,\n",
    " 'eval_runtime': 356.2403,\n",
    " 'eval_samples_per_second': 70.177,\n",
    " 'eval_steps_per_second': 17.544,\n",
    " 'epoch': 1.0}\n",
    " \n",
    "The model with a finetuned classification head and the one further finetuned with LoRA are more accurate than the pretrained model adapted with linear probing without finetuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
